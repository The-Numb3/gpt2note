from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path
from datetime import datetime
import os
import json
import re
import httpx
from typing import List, Dict, Any, Tuple, Optional

app = FastAPI()

# === CORS (ê°œë°œ ì¤‘ì€ * í—ˆìš©) ===
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,  # "*"ì™€ í•¨ê»˜ TrueëŠ” ë¸Œë¼ìš°ì €ì—ì„œ ë§‰í ìˆ˜ ìˆìŒ
    allow_methods=["*"],
    allow_headers=["*"],
)

# === ì„¤ì •ê°’ ===
# Obsidian Vault ê²½ë¡œ
VAULT_PATH = Path(os.environ.get("OBSIDIAN_VAULT_DIR", r"C:/ê¹€ê°•ë¯¼/obsidian_vault/vineyard"))

# ë¡œì»¬ LLM ì‚¬ìš©ì—¬ë¶€/ì—”ë“œí¬ì¸íŠ¸ (Ollama OpenAI-í˜¸í™˜ /v1/chat/completions ê¶Œì¥)
USE_LOCAL_LLM = os.environ.get("USE_LOCAL_LLM", "true").lower() == "true"
LOCAL_LLM_BASE_URL = os.environ.get("LOCAL_LLM_BASE_URL", "http://localhost:11434/v1")  # Ollama ê¸°ë³¸
LOCAL_LLM_MODEL = os.environ.get("LOCAL_LLM_MODEL", "llama3.1:8b-instruct-q4_K_M")
LOCAL_LLM_TEMPERATURE = float(os.environ.get("LOCAL_LLM_TEMPERATURE", "0.2"))
LOCAL_LLM_MAX_TOKENS = int(os.environ.get("LOCAL_LLM_MAX_TOKENS", "1600"))

# --- ìœ í‹¸ ---
def slugify(s: str) -> str:
    s = re.sub(r"[^\w\s\-\.]", "", s, flags=re.UNICODE)
    s = re.sub(r"\s+", "_", s, flags=re.UNICODE)
    return s.strip("_")[:80] or "note"

def ensure_project_dir(project: str) -> Path:
    p = VAULT_PATH / project
    p.mkdir(parents=True, exist_ok=True)
    return p

def to_frontmatter(meta: Dict[str, Any]) -> str:
    return "---\n" + "\n".join(f'{k}: {json.dumps(v, ensure_ascii=False)}' for k, v in meta.items()) + "\n---\n"

def build_basic_markdown(project: str, conversation: List[Dict[str, str]]) -> str:
    md_conv = []
    for msg in conversation:
        role = "ğŸ‘¤ User" if msg.get("role") == "user" else "ğŸ¤– Assistant"
        md_conv.append(f"### {role}\n{msg.get('content','')}\n")
    conv_md = "\n---\n".join(md_conv)
    return f"""# ğŸ“ Chat Conversation Report
**í”„ë¡œì íŠ¸**: {project}  
**ë‚ ì§œ**: {datetime.now().strftime('%Y-%m-%d')}  
**ëŒ€í™” ê¸¸ì´**: {len(conversation)} turns  

---

## ğŸ’¬ ì›ë³¸ ëŒ€í™” ê¸°ë¡
{conv_md}
""".strip()

# --- í”„ë¡¬í”„íŠ¸ ---
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ëŒ€í™” ë…¸íŠ¸ë¥¼ Obsidianì— ì €ì¥í•˜ê¸° ì¢‹ê²Œ 'ìš”ì•½/êµ¬ì¡°í™”/í•˜ì´ë¼ì´íŠ¸'í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
- ì¶œë ¥ì€ ë‘ ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆ„ì„¸ìš”:
====JSON====
{...ë©”íƒ€ë°ì´í„° JSON...}
====MARKDOWN====
...ì •ë¦¬ëœ ë§ˆí¬ë‹¤ìš´...
- JSONì€ ë°˜ë“œì‹œ ìœ íš¨í•œ JSON ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.
- MARKDOWNì€ Obsidian ì¹œí™”ì ìœ¼ë¡œ #, ##, - ëª©ë¡, ``` ì½”ë“œë¸”ë¡ ë“±ì„ í™œìš©í•˜ì„¸ìš”.
- í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""

USER_PROMPT_TEMPLATE = """ë‹¤ìŒ ëŒ€í™”ë¥¼ 'í”„ë¡œì íŠ¸ ë…¸íŠ¸'ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1) ìƒë‹¨ 'ìš”ì•½' (í•µì‹¬ í¬ì¸íŠ¸ 5ì¤„ ì´ë‚´)
2) 'ë°°ìš´ ì  / ê²°ë¡ '
3) 'ì•½í•œ ê°œë…/ì˜¤ê°œë…' íƒì§€ ë° 'í•™ìŠµ ê°€ì´ë“œ'(ë§í¬ ì—†ì´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸)
4) 'ë‹¤ìŒ í•  ì¼'(ì²´í¬ë°•ìŠ¤ ëª©ë¡)
5) ì›í•œë‹¤ë©´ ì§§ì€ 'íƒœê·¸' ì œì•ˆ (ì˜ˆ: #ì„ í˜•ëŒ€ìˆ˜, #ë²¡í„°)

ëŒ€í™”:
{conversation_text}

ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì´:
====JSON====
{{
  "title": "{title_hint}",
  "tags": ["ai", "notes"],
  "weak_topics": ["..."],
  "todo": ["..."]
}}
====MARKDOWN====
# {title_hint}
...ë³¸ë¬¸...
"""

def conversation_to_plaintext(convo: List[Dict[str, str]], max_chars: int = 12000) -> str:
    lines = []
    total = 0
    for m in convo:
        role = m.get("role", "user")
        content = (m.get("content") or "").strip()
        line = f"[{role}] {content}"
        if total + len(line) > max_chars:
            break
        lines.append(line)
        total += len(line)
    return "\n".join(lines)

async def call_local_llm(conversation: List[Dict[str, str]], project: str) -> Tuple[Optional[Dict[str, Any]], Optional[str], str]:
    """
    ë¡œì»¬ LLM(OpenAI-í˜¸í™˜ /v1/chat/completions) í˜¸ì¶œ â†’ (meta_json, markdown, raw_text)
    ì‹¤íŒ¨ì‹œ (None, None, raw_text)
    """
    title_hint = f"{project} - {datetime.now().strftime('%Y-%m-%d')}"
    convo_text = conversation_to_plaintext(conversation)

    user_prompt = USER_PROMPT_TEMPLATE.format(conversation_text=convo_text, title_hint=title_hint)

    payload = {
        "model": LOCAL_LLM_MODEL,
        "temperature": LOCAL_LLM_TEMPERATURE,
        "max_tokens": LOCAL_LLM_MAX_TOKENS,
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ]
    }

    url = f"{LOCAL_LLM_BASE_URL}/chat/completions"  # Ollama OpenAI-compat ê²½ë¡œ
    try:
        async with httpx.AsyncClient(timeout=120) as client:
            r = await client.post(url, json=payload)
            r.raise_for_status()
            data = r.json()
            text = data["choices"][0]["message"]["content"]
    except Exception as e:
        # ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ë§ˆí¬ë‹¤ìš´ë§Œ ì‚¬ìš©í•˜ë„ë¡ raw ë°˜í™˜
        return None, None, f"[LLM í˜¸ì¶œ ì‹¤íŒ¨] {e}"

    # ë¶„ë¦¬ íŒŒì‹±
    meta_json, md = None, None
    if "====JSON====" in text and "====MARKDOWN====" in text:
        try:
            parts = text.split("====JSON====", 1)[1]
            meta_part, md_part = parts.split("====MARKDOWN====", 1)
            meta_json = json.loads(meta_part.strip())
            md = md_part.strip()
        except Exception:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í†µìœ¼ë¡œ md ì·¨ê¸‰
            md = text.strip()
    else:
        md = text.strip()

    return meta_json, md, text

# --- ë¼ìš°íŠ¸ ---

@app.get("/health")
def health():
    return {"ok": True}

@app.post("/api/conversation/analyze")
async def analyze_only(req: Request):
    """
    ëŒ€í™”ë¥¼ ë¶„ì„ë§Œ í•´ì„œ meta + markdownì„ ë°˜í™˜ (ì €ì¥ì€ ì•ˆ í•¨)
    """
    data = await req.json()
    project = data.get("project", "General")
    conversation = data.get("conversation", [])

    if USE_LOCAL_LLM:
        meta, md, raw = await call_local_llm(conversation, project)
    else:
        # ì—¬ê¸°ì„œ OpenAI ë“± ì™¸ë¶€ API ë¶„ê¸° ê°€ëŠ¥ (í˜„ì¬ëŠ” ë¡œì»¬ë§Œ)
        meta, md, raw = await call_local_llm(conversation, project)

    if md is None:
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë§ˆí¬ë‹¤ìš´ë§Œ ìƒì„±
        md = build_basic_markdown(project, conversation)

    return {
        "meta": meta or {},
        "markdown": md
    }

@app.post("/api/conversation/save+analyze")
async def save_and_analyze(req: Request):
    """
    ëŒ€í™”ë¥¼ ë¶„ì„í•˜ê³ , Obsidian Vaultì— ì €ì¥
    - frontmatter(meta) + markdown ë³¸ë¬¸
    """
    data = await req.json()
    project = data.get("project", "General")
    source = data.get("source", "extension")
    conversation = data.get("conversation", [])

    # ë¶„ì„
    if USE_LOCAL_LLM:
        meta, md, raw = await call_local_llm(conversation, project)
    else:
        meta, md, raw = await call_local_llm(conversation, project)

    # ë©”íƒ€ ê¸°ë³¸ê°’
    meta = meta or {}
    meta.setdefault("project", project)
    meta.setdefault("source", source)
    meta.setdefault("saved_at", datetime.now().isoformat(timespec="seconds"))

    # ë³¸ë¬¸ í™•ë³´(ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í…œí”Œë¦¿)
    if md is None:
        md = build_basic_markdown(project, conversation)

    # íŒŒì¼ ì €ì¥
    project_dir = ensure_project_dir(project)
    title = meta.get("title") or f"{project} - {datetime.now().strftime('%Y-%m-%d')}"
    fname = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{slugify(title)}.md"

    content = to_frontmatter(meta) + "\n" + md.strip() + "\n"

    file_path = project_dir / fname
    file_path.write_text(content, encoding="utf-8")

    return {"status": "success", "file": str(file_path), "meta": meta}

# === í•˜ìœ„í˜¸í™˜: ê¸°ì¡´ ê²½ë¡œ ìœ ì§€ ===
@app.post("/api/conversation/save")
async def save_conversation_legacy(req: Request):
    """
    ê¸°ì¡´ í™•ì¥ì´ í˜¸ì¶œí•˜ë˜ ë‹¨ìˆœ ì €ì¥ ì—”ë“œí¬ì¸íŠ¸(ë¶„ì„ ì—†ì´)
    """
    data = await req.json()
    project = data.get("project", "General")
    conversation = data.get("conversation", [])

    project_path = ensure_project_dir(project)

    md = build_basic_markdown(project, conversation)
    filename = project_path / f"conversation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    filename.write_text(md, encoding="utf-8")
    return {"status": "success", "file": str(filename)}
