# main.py
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
import httpx
import json
import os
import re

app = FastAPI()

# === CORS ===
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],          # ê°œë°œ ì¤‘ì€ * í—ˆìš©
    allow_credentials=False,      # "*"ì™€ í•¨ê»˜ TrueëŠ” ë¸Œë¼ìš°ì €ì—ì„œ ë§‰í ìˆ˜ ìˆìŒ
    allow_methods=["*"],
    allow_headers=["*"],
)

# === í™˜ê²½/ì„¤ì • ===
# ê¸°ë³¸ ì €ì¥ ê²½ë¡œ: Obsidian ì—†ì–´ë„ ê°œë°œ í´ë”ë¡œ ë™ì‘
VAULT_PATH = Path(
    os.environ.get(
        "OBSIDIAN_VAULT_DIR",
        str(Path.home() / "ObsidianVaultDev")  # ê°œë°œìš© ê¸°ë³¸
    )
)

# ë¡œì»¬ LLM (Ollama) ê¸°ë³¸ ì„¤ì •
LOCAL_LLM_BASE_URL = os.environ.get("LOCAL_LLM_BASE_URL", "http://localhost:11434")
LOCAL_LLM_MODEL = os.environ.get("LOCAL_LLM_MODEL", "llama3.1:8b-instruct-q4_K_M")
LOCAL_LLM_TEMPERATURE = float(os.environ.get("LOCAL_LLM_TEMPERATURE", "0.2"))
LOCAL_LLM_MAX_TOKENS = int(os.environ.get("LOCAL_LLM_MAX_TOKENS", "1600"))

# === ìœ í‹¸ ===
def slugify(s: str) -> str:
    s = re.sub(r"[^\w\s\-\.]", "", s, flags=re.UNICODE)
    s = re.sub(r"\s+", "_", s, flags=re.UNICODE)
    return s.strip("_")[:80] or "note"

def ensure_dir(p: Path) -> Path:
    if not p.exists():
        try:
            p.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"ê²½ë¡œë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {p} ({e})")
    return p

def to_frontmatter(meta: Dict[str, Any]) -> str:
    # ëª¨ë“  ê°’ì„ JSONìœ¼ë¡œ ì§ë ¬í™”í•˜ì—¬ í•œê¸€/íŠ¹ìˆ˜ë¬¸ì ë³´ì¡´
    return "---\n" + "\n".join(
        f"{k}: {json.dumps(v, ensure_ascii=False)}" for k, v in meta.items()
    ) + "\n---\n"

def build_basic_markdown(project: str, conversation: List[Dict[str, str]]) -> str:
    md_conv = []
    for msg in conversation:
        role = msg.get("role", "")
        content = (msg.get("content") or "").strip()
        role_md = "ğŸ‘¤ User" if role == "user" else "ğŸ¤– Assistant"
        md_conv.append(f"### {role_md}\n{content}\n")
    conv_md = "\n---\n".join(md_conv)
    return f"""# ğŸ“ Chat Conversation Report
**í”„ë¡œì íŠ¸**: {project}  
**ë‚ ì§œ**: {datetime.now().strftime('%Y-%m-%d')}  
**ëŒ€í™” ê¸¸ì´**: {len(conversation)} turns  

---

## ğŸ’¬ ì›ë³¸ ëŒ€í™” ê¸°ë¡
{conv_md}
""".strip()

def conversation_to_plaintext(convo: List[Dict[str, str]], max_chars: int = 12000) -> str:
    lines, total = [], 0
    for m in convo:
        role = m.get("role", "user")
        content = (m.get("content") or "").strip()
        line = f"[{role}] {content}"
        if total + len(line) > max_chars:
            break
        lines.append(line)
        total += len(line)
    return "\n".join(lines)

# === í”„ë¡¬í”„íŠ¸ ===
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ëŒ€í™” ë…¸íŠ¸ë¥¼ Obsidianì— ì €ì¥í•˜ê¸° ì¢‹ê²Œ 'ìš”ì•½/êµ¬ì¡°í™”/í•˜ì´ë¼ì´íŠ¸'í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
- ì¶œë ¥ì€ ë‘ ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆ„ì„¸ìš”:
====JSON====
{...ë©”íƒ€ë°ì´í„° JSON...}
====MARKDOWN====
...ì •ë¦¬ëœ ë§ˆí¬ë‹¤ìš´...
- JSONì€ ë°˜ë“œì‹œ ìœ íš¨í•œ JSON ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.
- MARKDOWNì€ Obsidian ì¹œí™”ì ìœ¼ë¡œ #, ##, - ëª©ë¡, ``` ì½”ë“œë¸”ë¡ ë“±ì„ í™œìš©í•˜ì„¸ìš”.
- í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""

USER_PROMPT_TEMPLATE = """ë‹¤ìŒ ëŒ€í™”ë¥¼ 'í”„ë¡œì íŠ¸ ë…¸íŠ¸'ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1) ìƒë‹¨ 'ìš”ì•½' (í•µì‹¬ í¬ì¸íŠ¸ 5ì¤„ ì´ë‚´)
2) 'ë°°ìš´ ì  / ê²°ë¡ '
3) 'ì•½í•œ ê°œë…/ì˜¤ê°œë…' íƒì§€ ë° 'í•™ìŠµ ê°€ì´ë“œ'(ë§í¬ ì—†ì´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸)
4) 'ë‹¤ìŒ í•  ì¼'(ì²´í¬ë°•ìŠ¤ ëª©ë¡)
5) ì§§ì€ 'íƒœê·¸' ì œì•ˆ (ì˜ˆ: #ì„ í˜•ëŒ€ìˆ˜, #ë²¡í„°)

ëŒ€í™”:
{conversation_text}

ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì´:
====JSON====
{{
  "title": "{title_hint}",
  "tags": ["ai", "notes"],
  "weak_topics": ["..."],
  "todo": ["..."]
}}
====MARKDOWN====
# {title_hint}
...ë³¸ë¬¸...
"""

# === LLM í˜¸ì¶œ (OpenAI-í˜¸í™˜ â†’ ì‹¤íŒ¨ì‹œ Ollama ë„¤ì´í‹°ë¸Œ) ===
async def _try_openai_compat(client: httpx.AsyncClient, model: str, sys_prompt: str, user_prompt: str) -> Optional[str]:
    url = f"{LOCAL_LLM_BASE_URL.rstrip('/')}/v1/chat/completions"
    payload = {
        "model": model,
        "temperature": LOCAL_LLM_TEMPERATURE,
        "max_tokens": LOCAL_LLM_MAX_TOKENS,
        "messages": [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": user_prompt},
        ],
    }
    r = await client.post(url, json=payload)
    if r.status_code >= 400:
        return None
    data = r.json()
    try:
        return data["choices"][0]["message"]["content"].strip()
    except Exception:
        return None

async def _try_ollama_native(client: httpx.AsyncClient, model: str, sys_prompt: str, user_prompt: str) -> Optional[str]:
    # https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion
    url = f"{LOCAL_LLM_BASE_URL.rstrip('/')}/api/chat"
    payload = {
        "model": model,
        "options": {
            "temperature": LOCAL_LLM_TEMPERATURE,
            "num_predict": LOCAL_LLM_MAX_TOKENS
        },
        "messages": [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "stream": False
    }
    r = await client.post(url, json=payload)
    if r.status_code >= 400:
        return None
    data = r.json()
    try:
        return (data.get("message", {}) or {}).get("content", "").strip()
    except Exception:
        return None

async def call_local_llm(conversation: List[Dict[str, str]], project: str) -> Tuple[Optional[Dict[str, Any]], Optional[str], str]:
    """
    LLM í˜¸ì¶œ â†’ (meta_json, markdown, raw_text)
    - OpenAI-compat ì‹¤íŒ¨ ì‹œ Ollama nativeë¡œ ì¬ì‹œë„
    - íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ Markdownìœ¼ë¡œ ì‚¬ìš©
    - ì™„ì „ ë¹ˆ ì‘ë‹µ ë°©ì§€
    """
    title_hint = f"{project} - {datetime.now().strftime('%Y-%m-%d')}"
    convo_text = conversation_to_plaintext(conversation)
    user_prompt = USER_PROMPT_TEMPLATE.format(conversation_text=convo_text, title_hint=title_hint)

    text = ""
    try:
        async with httpx.AsyncClient(timeout=180) as client:
            text = await _try_openai_compat(client, LOCAL_LLM_MODEL, SYSTEM_PROMPT, user_prompt) or ""
            if not text:
                text = await _try_ollama_native(client, LOCAL_LLM_MODEL, SYSTEM_PROMPT, user_prompt) or ""
    except Exception as e:
        text = f"[LLM í˜¸ì¶œ ì‹¤íŒ¨] {e}"

    # íŒŒì‹±
    meta_json, md = None, None
    if "====JSON====" in text and "====MARKDOWN====" in text:
        try:
            parts = text.split("====JSON====", 1)[1]
            meta_part, md_part = parts.split("====MARKDOWN====", 1)
            meta_json = json.loads(meta_part.strip())
            md = md_part.strip()
        except Exception:
            md = text.strip()
    else:
        md = text.strip()

    if not md:
        md = "## ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n- ëª¨ë¸ ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

    return meta_json, md, text

# === ë¼ìš°íŠ¸ ===
@app.get("/health")
def health():
    return {"ok": True, "vault": str(VAULT_PATH), "model": LOCAL_LLM_MODEL}

@app.post("/api/conversation/analyze")
async def analyze_only(req: Request):
    """
    ì €ì¥ ì—†ì´ ë¶„ì„ ê²°ê³¼ë§Œ ë°˜í™˜
    """
    data = await req.json()
    project = data.get("project", "General")
    conversation = data.get("conversation", [])

    meta, md, raw = await call_local_llm(conversation, project)
    if not md or not md.strip():
        md = build_basic_markdown(project, conversation)

    return {"meta": meta or {}, "markdown": md}

@app.post("/api/conversation/save+analyze")
async def save_and_analyze(req: Request):
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (frontmatter + markdown ë³¸ë¬¸)
    - ìš”ì²­ bodyì—ì„œ vault_dirê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    """
    data = await req.json()
    project = data.get("project", "General")
    source = data.get("source", "extension")
    conversation = data.get("conversation", [])
    vault_override = data.get("vault_dir")

    vault_dir = Path(vault_override) if vault_override else VAULT_PATH
    ensure_dir(vault_dir)
    project_dir = ensure_dir(vault_dir / project)

    # ë¶„ì„
    meta, md, raw = await call_local_llm(conversation, project)

    # ë©”íƒ€ ë³´ê°•
    meta = meta or {}
    meta.setdefault("title", f"{project} - {datetime.now().strftime('%Y-%m-%d')}")
    meta.setdefault("project", project)
    meta.setdefault("created", datetime.utcnow().isoformat())
    meta.setdefault("tags", [])
    meta.setdefault("source", source)
    meta.setdefault("turns", len(conversation))

    # ë³¸ë¬¸ ë³´ê°•: ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ëŒ€ì²´
    if not md or not md.strip():
        md = build_basic_markdown(project, conversation)

    # íŒŒì¼ ì €ì¥
    title = meta.get("title") or f"{project} - {datetime.now().strftime('%Y-%m-%d')}"
    fname = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{slugify(title)}.md"
    content = to_frontmatter(meta) + "\n" + md.strip() + "\n"

    file_path = project_dir / fname
    file_path.write_text(content, encoding="utf-8")

    return {"status": "success", "file": str(file_path), "meta": meta}

# í•˜ìœ„í˜¸í™˜: ë¶„ì„ ì—†ì´ ì €ì¥
@app.post("/api/conversation/save")
async def save_conversation_legacy(req: Request):
    data = await req.json()
    project = data.get("project", "General")
    conversation = data.get("conversation", [])
    vault_override = data.get("vault_dir")

    vault_dir = Path(vault_override) if vault_override else VAULT_PATH
    ensure_dir(vault_dir)
    project_dir = ensure_dir(vault_dir / project)

    md = build_basic_markdown(project, conversation)
    filename = project_dir / f"conversation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    filename.write_text(md, encoding="utf-8")
    return {"status": "success", "file": str(filename)}
